
```{r, include = FALSE}
ottrpal::set_knitr_image_path()
```
# Cromwell at the Fred Hutch

Cromwell can help run WDL workflows on a variety of computing resources such as SLURM clusters, as well as AWS, Google and Azure cloud computing systems. 
Using WDL workflows allows users to focus on their workflow contents rather than the intricacies of the particular computing platforms they are using.  
However, there are always optimizations of how those workflows run taht may be somewhat specific to the various computing tools you may be using.  
We'll discuss some of the available customizations to help you run WDLs on our cluster in a simple way that still allows those workflows to be portable to other computing platforms.  


## Standard Runtime Variables
These runtime variables are both the defaults for our Fred Hutch configuration and also standard runtime variables you can use on other computing platforms too. 
- `cpu = 1`
  - An integer number of cpus you want for the task
- `memory = 2000`
  - An integer number of MB of memory you want to use for the task


## Fred Hutch Custom Runtime Variables
For the `gizmo` cluster, the following runtime variables are available that are customized to our configuration.  
What is specified below is the current default as written, you can edit these in the config file if you'd like OR you can specify these variables in your `runtime` block in each task to change only the variables you want to change from the default for that particular task.  


- `walltime = "18:00:00"`
  - A string of date/time that specifies how many hours/days you want to request for the task
- `partition = "campus-new"`
  - Which partition you want to use, the default is `campus-new` but whatever is in the runtime block of your WDL will overrride this. 
- `modules = ""`
  - A space-separated list of the environment modules you'd like to have loaded (in that order) prior to running the task.  
- `docker = "ubuntu:latest"`
  - A specific Docker container to use for the task.  For the custom Hutch configuration, docker containers can be specified and the necessary conversions (to Singularity) will be performed by Cromwell (not the user).  Note: when docker is used, soft links cannot be used in our filesystem, so workflows using very large datasets may run slightly slower due to the need for Cromwell to copy files rather than link to them.  
- `dockerSL = "ubuntu:latest"`
  - This is a custom configuration for the Hutch that allows users to use docker and softlinks only to specific locations in Scratch.  It is helpful when working with very large files. 
- `account = "paguirigan_a"`
  - This allows users who run jobs for multiple PI accounts to specify at the level of a task which account to use for a given job to manage cluster allocations.  




## Running Workflows

### Modules vs Docker
- cloud will require docker but working locally does not
- docker requires copying of inputs (does not allow for softlinks) so when using big data inputs this can be slower than using modules/softlinks

### Managing Results
Best practices include:
- using Scratch for all work in progress
- copy only required, final, archivable workflow outputs to Fast or AWS S3 using workflow options
- copy WDL, and any input json or workflow options files to same directory as archived results for a "complete" data package
- commit code to GitHub and consider versioning/creating a release to more thoroughly document and share code associated with the above data package. 




## Guidance and Support
### Effective Computing User Group
This is a drop-in style biweekly meeting that provides resources for folks to talk about their computational work and improve over time.  Find current events by checking out the FH-Data Slack workspace [here](https://fhdata.slack.com).

